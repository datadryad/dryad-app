Notes on Amazon AWS
====================

AWS troubleshooting
=====================

If you’re trying to connect to something and get no response, check
the firewall. Identify the machine’s security group, go to EC2
settings for that security group, and ensure they allow the activity
you’re trying.

AWS Client Setup
==================

How to install the AWS client on a local computer. The client is already installed on EC2 when
using the Amazon Linux.

1. Install the AWS command-line tools.
   * On OSX, this is `sudo pip install awscli --ignore-installed six`
   * [Instructions for other OS’s](http://docs.aws.amazon.com/cli/latest/userguide/installing.html#install-bundle-other-os)
2. Configure the command-line tools
   * First, you need an access key ID and secret generated by the IAM service.
   * `aws configure`
   * enter your ID and secret
   * set the default region to `us-east-1`
   * leave the output format blank
    
# S3 for staged file uploads from the UI

We are moving to a model where staged uploads will be stored on S3 rather than
on EFS (which appears like a local disk to the server).

In order to set this up, each environment that we want to keep separate
will have its own bucket.

For access to these buckets within the UI, we need to configure a few things as shown
below for *each environment* that we need to have separate uploads (probably development, stage and production).

```yaml
  s3:
    region: us-west-2
    bucket: <bucketname>
    key: <key_id>
    secret: <%= Rails.application.credentials[Rails.env.to_sym][:s3_secret] %>
```

We also need to configure some Cross-origin request sharing (CORS) policies
within Amazon S3 in order to allow uploads across domains
with javascript with the EvaporateJS library. We need to expose the eTag for Evaporate to work.

Each domain that needs to communicate with the S3 bucket for uploads
needs to be added to the CORS policy for the bucket under
S3 > *bucketname* > Permissions > Cross-origin resource sharing (toward the
bottom of that page).

```json
[
    {
        "AllowedHeaders": [
            "*"
        ],
        "AllowedMethods": [
            "PUT",
            "POST",
            "DELETE",
            "GET"
        ],
        "AllowedOrigins": [
            "http://localhost:3000"
        ],
        "ExposeHeaders": [
            "ETag"
        ],
        "MaxAgeSeconds": 3600
    }
]
```

The buckets for local-server development environments have an additional hash added based
on the machine or server name.  This keeps multiple developers from interfering with the
storage (and possibly duplicate resource IDs in different local databases and collisions).
These look like 2c6f1ccc-25 (the first part is a shortened hash/digest and the part after the dash
is the resource id).

See the method `resource.s3_dir_name` to understand how this works.  You can also run a rails console
and use this method to see what the hash is for your machine and a resource
`StashEngine::Resource.find(<id>).s3_dir_name`.


EvaporateJS works by making signing requests against an AJAX URL inside the Rails
application, and so the
[:s3][:secret] is never exposed in any client side code, but only the results
of the signing requests. It is then able
to upload directly to S3 from the user's browser rather than going through
an intermediary server.  It also splits uploads into multiple parts so it
can handle large (> 5GB) uploads which are not supported by single s3 requests.

The library is set up using npm modules.

## Updating node and Evaporate js libraries

The evaporate JS library is updated using yarn as a NPM library. (No longer browserify since we
transitioned it to React code.)


Working With S3
=================

Listing bucket contents
`aws s3 ls s3://dryad-upload/`

Copying to a bucket
`aws s3 cp myfile.txt s3://dryad-upload`

Copying contents of a bucket
`aws s3 sync s3://mybucket s3://mybucket2`

(or look in the aws UI console if you have access)

The AWS Frictionless Lambda
===========================

The lambda for frictionless uses a very simple model that doesn't maintain any internal
state from one run to another and doesn't access other AWS internal APIs except for
the default logging.
All specific state information it needs per processing request
is passed into the Lambda and results are returned to the callback URL that is passed in.

INPUT json example:

```json
{
  "download_url": "https://dryad-s3-prd.s3.us-west-2.amazonaws.com/196263/data/sampleckey.xlsx?X-Amz-Algo...",
  "callback_url": "https://datadryad.org/api/v2/files/1814997/frictionlessReport",
  "token": "trQDojaI2qSZegJIUBEONSu69WREIFUuFNDtmECGM1I"
}
```

- Although the **download\_url** in this example looks like an S3 url, it is a *presigned url*
  which works the same as any HTTP url on the internet that responds to a GET request (during the time it is valid).
- The **callback\_url** is calculated and then passed in for the current environment to access our own API for updates.
- The **token** is also pre-calculated by our ruby code and passed in and will be valid for writing to our own API.

## The easy way to test the lambda using a non-public server

- Connect to the VPN or use sshuttle so you have access to our standard development database.
- Start your development environment with `rails s -e local_dev` (add `bundle exec` before this command if needed).
- This automatically has correct API info filled into the database and callback URLs are directed to our
  standard development server which will then make updates to the same database that this environment uses.

## I want to test the lambda the harder way with a web accessible server
- Set up a server that is publicly accessible on the internet.
- Go to *app/config/environments* and add a new file for your new environment
  - Change the settings for `config.action_mailer.default_url_options` and 
    `Rails.application.default_url_options` so it has the correct hostname (and port if needed) to
    create correct URLs for your environment.  See the other environment files for examples
    (especially the development environment).
- Add the environment to other config files (it might inherit most settings from development).
  - `app_config.yml`
  - `database.yml`
  - `tenants/*.yml` (You may only need to add your special environment to a few tenants you intend to use,
    but I haven't tested this.)
- Add an API account with permission to write information. (see `documentation/apis/adding_api_accounts.md`
  and `documentation/apis/choosing_authentication_type.md` and the grant type will be Client Credentials
  Grant in this case and based on a user with appropriate permissions to write for all datasets.)
- In the table `stash_engine_api_tokens` add a row and add the `app_id` and `secret` for the user account above.
  For the `expires_at`, `created_at` and `updated_at` dates, type in a date at least a few days old.
  Save the row.  This will be used to generate the tokens for our API that will be used with the
  callback and our code will keep them up to date and create new tokens when one is about to expire.
  Our code uses the public doorkeeper (http) API to obtain/update tokens to avoid against internal
  changes and using internal methods in the doorkeeper library.


Working With EC2
==================

Spinning up an EC2 machine
* Use the Amazon Linux unless you have a good reason to do otherwise.
* It’s not necessary to add extra storage beyond the default, you can
  always add extra disks later.
* Tag the instance with a name that matches its purpose.
* Create a new keypair.

Logging into an EC2 machine
* move the keypair to somewhere you can access
* get the public DNS name of the machine from the EC2 console;
  e.g. `ec2-52-90-82-228.compute-1.amazonaws.com`
* `ssh -i myKeypair.pem ec2-user@ec2-52-90-82-228.compute-1.amazonaws.com`

Generating a keypair
* `aws ec2 create-key-pair --key-name MyKeyPair`

Server permissions and access control with IAM
=================================================
In general, user permissions are set up in IAM, and OpsWorks
automatically applies them to the appropriate servers.

Users can login to servers with a command like:
`ssh -i ~/.ssh/[your-keyfile] someuser@machinename.datadryad.org`

About IAM

* Users belong to groups
* Groups get policies
* Policies contain access control rules
* When accessing the AWS website, IAM users must login via the special
  “IAM console link” on the administrator’s IAM dashboard. Dryad’s login
  link: https://datadryad.signin.aws.amazon.com/console
* Roles are a way for a single user to have multiple sets of
  permissions, sort of like sudo. They can be useful for allowing
  external applications to access a set of resources.
* To register a new user
  * Log in to the AWS website as root (from the main login screen, follow
    the link near the bottom)
  * Go to the IAM section. Create the new user there (make sure it’s in
    the correct region) and give them console permissions and ssh
	permissions and add them to the Developers access group.
  * Then go to the OpsWorks console. From there, you should be able to
    edit users and import new IAM users from the region. The new user
    should then show up and be editable. You can turn on self-management
    of the user there, allowing them to set their own ssh public key.
  * You need to then go to the OpsWorks stack and “run command” with the
    command “configure” to goose the various servers into recognizing the
    new user, but after that, the user should be able to ssh to any of
    our AWS servers.

Monitoring
==========

Monitoring is performed by Amazon CloudWatch. URL-based alarms can be
created and managed through Route 53, but these appear in CloudWatch,
and CloudWatch is required to manage anything that cannot be
represented as a URL.

Most alarms are tied to the DryadAdmin notifications. When a
notification is sent by one of these alarms, it goes to Slack, the
admin email, and developers' phones.

 
Curator PC
==============

Dryad maintains a [virtual PC for curator use](curator_pc.md) on AWS.

